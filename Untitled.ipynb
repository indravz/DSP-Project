{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/\n",
    "#https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.transform import four_point_transform\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ pip install --upgrade imutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description: In order to speed up and for accurate edge detection, we resize the scanned images to have 500 pixels\n",
    "# Ratio is kept in a variable to create original image again\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walmart.10.jpg']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath='ourd/training_set/images'\n",
    "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "print(onlyfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ourd/training_set/imagesWalmart.10.jpg\n",
      "(3264, 2448, 3)\n",
      "(500, 375, 3)\n",
      "Edge Detection\n",
      "STEP 2: Find contours of paper\n",
      "STEP 3: Apply perspective transform\n"
     ]
    }
   ],
   "source": [
    "for imglist in onlyfiles:\n",
    "    print(mypath+imglist)\n",
    "    image = cv2.imread(mypath+'/'+imglist)\n",
    "    print(image.shape)\n",
    "    ratio = image.shape[0] / 500.0 # Ratio of original to new height\n",
    "    orig = image.copy()\n",
    "    image = imutils.resize(image, height = 500)\n",
    "    print(image.shape)\n",
    "    # convert the image to grayscale, blur it, and find edges\n",
    "    # in the image\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "    print(\"Edge Detection\")\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.imshow(\"Edged\", edged)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    #cv2.imshow(\"Image\", image)\n",
    "    #cv2.imshow(\"Edged\", edged)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "         # approximate the contour\n",
    "         peri = cv2.arcLength(c, True)\n",
    "         approx = cv2.approxPolyDP(c, 0.01 * peri, True)\n",
    " \n",
    "         # if our approximated contour has four points, then we\n",
    "         # can assume that we have found our screen\n",
    "         if len(approx) == 4:\n",
    "                  screenCnt = approx\n",
    "                  break\n",
    "    print(\"STEP 2: Find contours of paper\")\n",
    "    cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Outline\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "    warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    #cv2.imwrite(mypath+'/'+'new'+imglist,warped)\n",
    "    print(\"STEP 3: Apply perspective transform\")\n",
    "    cv2.imshow(\"Original\", imutils.resize(orig, height = 650))\n",
    "    cv2.imshow(\"Scanned\", imutils.resize(warped, height = 650))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold_local(image, block_size, method='gaussian', offset=0, mode='reflect', param=None, cval=0)[source]\n",
    "Compute a threshold mask image based on local pixel neighborhood.\n",
    "\n",
    "Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the ‘generic’ method.\n",
    "\n",
    "Parameters\n",
    "image(N, M) ndarray\n",
    "Input image.\n",
    "\n",
    "block_sizeint\n",
    "Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, …, 21, …).\n",
    "\n",
    "method{‘generic’, ‘gaussian’, ‘mean’, ‘median’}, optional\n",
    "Method used to determine adaptive threshold for local neighbourhood in weighted mean image.\n",
    "\n",
    "‘generic’: use custom function (see param parameter)\n",
    "\n",
    "‘gaussian’: apply gaussian filter (see param parameter for custom sigma value)\n",
    "\n",
    "‘mean’: apply arithmetic mean filter\n",
    "\n",
    "‘median’: apply median rank filter\n",
    "\n",
    "By default the ‘gaussian’ method is used.\n",
    "\n",
    "offsetfloat, optional\n",
    "Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.\n",
    "\n",
    "mode{‘reflect’, ‘constant’, ‘nearest’, ‘mirror’, ‘wrap’}, optional\n",
    "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to ‘constant’. Default is ‘reflect’.\n",
    "\n",
    "param{int, function}, optional\n",
    "Either specify sigma for ‘gaussian’ method or function object for ‘generic’ method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.\n",
    "\n",
    "cvalfloat, optional\n",
    "Value to fill past edges of input if mode is ‘constant’.\n",
    "\n",
    "Returns\n",
    "threshold(N, M) ndarray\n",
    "Threshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warped = (warped > T).astype(\"uint8\") * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the original and scanned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(\"STEP 3: Apply perspective transform\")\n",
    "#cv2.imshow(\"Original\", imutils.resize(orig, height = 650))\n",
    "#cv2.imshow(\"Scanned\", imutils.resize(warped, height = 650))\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Recognition with Opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(scores, geometry):\n",
    "         # grab the number of rows and columns from the scores volume, then\n",
    "         # initialize our set of bounding box rectangles and corresponding\n",
    "         # confidence scores\n",
    "         (numRows, numCols) = scores.shape[2:4]\n",
    "         rects = []\n",
    "         confidences = []\n",
    " \n",
    "         # loop over the number of rows\n",
    "         for y in range(0, numRows):\n",
    "                  # extract the scores (probabilities), followed by the\n",
    "                  # geometrical data used to derive potential bounding box\n",
    "                  # coordinates that surround text\n",
    "                  scoresData = scores[0, 0, y]\n",
    "                  xData0 = geometry[0, 0, y]\n",
    "                  xData1 = geometry[0, 1, y]\n",
    "                  xData2 = geometry[0, 2, y]\n",
    "                  xData3 = geometry[0, 3, y]\n",
    "                  anglesData = geometry[0, 4, y]\n",
    " \n",
    "                  # loop over the number of columns\n",
    "                  for x in range(0, numCols):\n",
    "                           # if our score does not have sufficient probability,\n",
    "                           # ignore it\n",
    "                           if scoresData[x] < 0.01:\n",
    "                                    continue\n",
    " \n",
    "                           # compute the offset factor as our resulting feature\n",
    "                           # maps will be 4x smaller than the input image\n",
    "                           (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    " \n",
    "                           # extract the rotation angle for the prediction and\n",
    "                           # then compute the sin and cosine\n",
    "                           angle = anglesData[x]\n",
    "                           cos = np.cos(angle)\n",
    "                           sin = np.sin(angle)\n",
    " \n",
    "                           # use the geometry volume to derive the width and height\n",
    "                           # of the bounding box\n",
    "                           h = xData0[x] + xData2[x]\n",
    "                           w = xData1[x] + xData3[x]\n",
    " \n",
    "                           # compute both the starting and ending (x, y)-coordinates\n",
    "                           # for the text prediction bounding box\n",
    "                           endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "                           endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "                           startX = int(endX - w)\n",
    "                           startY = int(endY - h)\n",
    " \n",
    "                           # add the bounding box coordinates and probability score\n",
    "                           # to our respective lists\n",
    "                           rects.append((startX, startY, endX, endY))\n",
    "                           confidences.append(scoresData[x])\n",
    " \n",
    "         # return a tuple of the bounding boxes and associated confidences\n",
    "         return (rects, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949, 981)\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"Publix.12.jpg\")\n",
    "orig = image.copy()\n",
    "(origH, origW) = image.shape[:2]\n",
    "print((origH, origW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new width and height and save the old to new ratio's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(newW, newH) = (320, 320)\n",
    "rW = origW / float(newW)\n",
    "rH = origH / float(newH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.resize(image, (newW, newH))\n",
    "(H, W) = image.shape[:2]\n",
    "print(image.shape)\n",
    "cv2.imshow(\"Image\",image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# East text detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerNames = [\n",
    "  \"feature_fusion/Conv_7/Sigmoid\",\n",
    "  \"feature_fusion/concat_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   ...\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]]\n",
      "\n",
      "  [[138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   ...\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]]\n",
      "\n",
      "  [[151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   ...\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]]]]\n",
      "First Blob: (1, 3, 320, 320)\n"
     ]
    }
   ],
   "source": [
    "blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "print(blob)\n",
    "print(\"First Blob: {}\".format(blob.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   ...\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]\n",
      "   [131.32 131.32 131.32 ... 131.32 131.32 131.32]]\n",
      "\n",
      "  [[138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   ...\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]\n",
      "   [138.22 138.22 138.22 ... 138.22 138.22 138.22]]\n",
      "\n",
      "  [[151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   ...\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]\n",
      "   [151.06 151.06 151.06 ... 151.06 151.06 151.06]]]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "keypoints=blob\n",
    "print(keypoints)\n",
    "img = image.copy()\n",
    "print(len(keypoints))\n",
    "#for x in range(0,len(keypoints)):\n",
    " # img2=cv2.circle(img, (np.int(keypoints[x].pt[0]),np.int(keypoints[x].pt[1])), radius=np.int(keypoints[x].size), color=(0,255,0), thickness=1000)\n",
    "#cv2.imshow(\"Image\",img2)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "net.setInput(blob)\n",
    "(scores, geometry) = net.forward(layerNames)\n",
    "#end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.object_detection import non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 14 116  59 132]\n",
      " [127 216 174 233]\n",
      " [129   7 152  26]\n",
      " [129  86 151 106]\n",
      " [164   8 190  25]\n",
      " [ 26 280  63 300]\n",
      " [ 29 153  67 171]\n",
      " [237  27 268  51]\n",
      " [ 77 194 113 210]\n",
      " [ 78 158 112 176]\n",
      " [240  87 262 125]\n",
      " [ 16 226  63 243]\n",
      " [ 80 100 105 120]\n",
      " [ 14  61  68  79]\n",
      " [ 70 286 118 301]\n",
      " [ 65 174  90 194]\n",
      " [ 39  42  72  61]\n",
      " [ 41  21  84  39]\n",
      " [ 36 191  72 210]\n",
      " [  2 267  24 282]\n",
      " [ 77 119 116 138]\n",
      " [ 70  41  99  61]\n",
      " [ 98   5 124  22]\n",
      " [120  25 146  46]\n",
      " [ 69 270 110 284]\n",
      " [ 12  96  46 115]\n",
      " [ 29 266  57 284]\n",
      " [ 13  22  36  41]\n",
      " [  7 205  39 226]\n",
      " [242 179 262 207]\n",
      " [104 103 127 125]\n",
      " [ 11  39  42  61]\n",
      " [247 163 263 184]\n",
      " [ 95  25 116  44]\n",
      " [247 294 264 309]\n",
      " [ 87 173 112 199]\n",
      " [242 234 264 255]\n",
      " [249 209 263 240]\n",
      " [132 100 150 125]\n",
      " [ 26  78  53  97]\n",
      " [ 42 172  63 190]\n",
      " [161  85 177 109]\n",
      " [ 50  80  73 101]]\n"
     ]
    }
   ],
   "source": [
    "(rects, confidences) = decode_predictions(scores, geometry)\n",
    "#print(confidences)\n",
    "boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (startX, startY, endX, endY) in boxes:\n",
    "         # scale the bounding box coordinates based on the respective\n",
    "         # ratios\n",
    "         \n",
    "         #dX = int((endX - startX))*0.8\n",
    "         #dY = int((endY - startY))*0.8\n",
    "         startX = int(startX * rW)\n",
    "         startY = int(startY * rH)\n",
    "         endX = int(endX * rW)\n",
    "         endY = int(endY * rH)\n",
    " \n",
    "         # draw the bounding box on the image\n",
    "         \n",
    "         #dX = int((endX - startX))*0.8\n",
    "         #dY = int((endY - startY))*0.8\n",
    "         \n",
    " \n",
    "         # apply padding to each side of the bounding box, respectively\n",
    "        # startX = max(0, startX - dX)\n",
    "        # startY = max(0, startY - dY)\n",
    "        # endX = min(origW, endX + (dX * 2))\n",
    "        # endY = min(origH, endY + (dY * 2))\n",
    "        # print(type(startX),startY)\n",
    "         cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2) \n",
    "        # extract the actual padded ROI\n",
    "         #roi = orig[startY:endY, startX:endX]\n",
    " \n",
    "# show the output image\n",
    "cv2.imshow(\"Text Detection\", orig)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of results\n",
    "results = []\n",
    " \n",
    "# loop over the bounding boxes\n",
    "for (startX, startY, endX, endY) in boxes:\n",
    "         # scale the bounding box coordinates based on the respective\n",
    "         # ratios\n",
    "         startX = int(startX * rW)\n",
    "         startY = int(startY * rH)\n",
    "         endX = int(endX * rW)\n",
    "         endY = int(endY * rH)\n",
    " \n",
    "         # in order to obtain a better OCR of the text we can potentially\n",
    "         # apply a bit of padding surrounding the bounding box -- here we\n",
    "         # are computing the deltas in both the x and y directions\n",
    "         dX = int((endX - startX) * 0.2)\n",
    "         dY = int((endY - startY) * 0.2)\n",
    " \n",
    "         # apply padding to each side of the bounding box, respectively\n",
    "         startX = max(0, startX - dX)\n",
    "         startY = max(0, startY - dY)\n",
    "         endX = min(origW, endX + (dX * 2))\n",
    "         endY = min(origH, endY + (dY * 2))\n",
    " \n",
    "         # extract the actual padded ROI\n",
    "         roi = orig[startY:endY, startX:endX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[254 254 254]\n",
      "  [255 255 255]\n",
      "  [253 253 253]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[253 253 253]\n",
      "  [255 255 255]\n",
      "  [251 251 251]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 254 254]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [255 255 255]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[254 252 252]\n",
      "  [255 255 255]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 254 254]\n",
      "  [255 255 255]\n",
      "  [253 253 253]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [252 252 252]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "print(roi)\n",
    "config = (\"-l eng --oem 1 --psm 7\")\n",
    "text = pytesseract.image_to_string(roi, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(((startX, startY, endX, endY), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR TEXT\n",
      "========\n",
      "The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sort the results bounding box coordinates from top to bottom\n",
    "results = sorted(results, key=lambda r:r[0][1])\n",
    " \n",
    "# loop over the results\n",
    "for ((startX, startY, endX, endY), text) in results:\n",
    "         # display the text OCR'd by Tesseract\n",
    "         print(\"OCR TEXT\")\n",
    "         print(\"========\")\n",
    "         print(\"{}\\n\".format(text))\n",
    " \n",
    "         # strip out non-ASCII text so we can draw the text on the image\n",
    "         # using OpenCV, then draw the text and a bounding box surrounding\n",
    "         # the text region of the input image\n",
    "         text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "         output = orig.copy()\n",
    "         cv2.rectangle(output, (startX, startY), (endX, endY),\n",
    "                  (0, 0, 255), 2)\n",
    "         cv2.putText(output, text, (startX, startY - 20),\n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    " \n",
    "         # show the output image\n",
    "         cv2.imshow(\"Text Detection\", output)\n",
    "         cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-e9ec2429c271>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m          \u001b[1;31m# extract the actual padded ROI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m          \u001b[0mroi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstartY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mendY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mendX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "for (startX, startY, endX, endY) in boxes:\n",
    "         # scale the bounding box coordinates based on the respective\n",
    "         # ratios\n",
    "         startX = int(startX * rW)\n",
    "         startY = int(startY * rH)\n",
    "         endX = int(endX * rW)\n",
    "         endY = int(endY * rH)\n",
    " \n",
    "         # in order to obtain a better OCR of the text we can potentially\n",
    "         # apply a bit of padding surrounding the bounding box -- here we\n",
    "         # are computing the deltas in both the x and y directions\n",
    "         dX = int((endX - startX))*0.8\n",
    "         dY = int((endY - startY))*0.8\n",
    " \n",
    "         # apply padding to each side of the bounding box, respectively\n",
    "         startX = max(0, startX - dX)\n",
    "         startY = max(0, startY - dY)\n",
    "         endX = min(origW, endX + (dX * 2))\n",
    "         endY = min(origH, endY + (dY * 2))\n",
    " \n",
    "         # extract the actual padded ROI\n",
    "         roi = orig[startY:endY, startX:endX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\"-l eng --oem 1 --psm 7\")\n",
    "text = pytesseract.image_to_string(roi, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
